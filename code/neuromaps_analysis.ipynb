{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from neuromaps.images import load_data, load_gifti, annot_to_gifti, relabel_gifti, construct_shape_gii\n",
    "from neuromaps.datasets import fetch_annotation\n",
    "from neuromaps.resampling import resample_images\n",
    "from neuromaps.nulls import alexander_bloch, burt2020\n",
    "from neuromaps.parcellate import Parcellater\n",
    "from scipy.stats import pearsonr\n",
    "from neuromaps import transforms \n",
    "from neuromaps.stats import compare_images\n",
    "from neuromaps.nulls import hungarian\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define path\n",
    "path = '/Users/laurituominen/Documents/Research/Reettis/neuromaps/'\n",
    "\n",
    "# load in different parcellation files\n",
    "dk_fsaverage_10k = (path + 'parcellations/atlas-desikankilliany_space-fsaverage_den-10k_hemi-L.label.gii.gz',\n",
    "                     path + 'parcellations/atlas-desikankilliany_space-fsaverage_den-10k_hemi-R.label.gii.gz')\n",
    "dk_fsaverage_164k = (path + 'parcellations/atlas-desikankilliany_space-fsaverage_den-164k_hemi-L.aparc-1.annot',\n",
    "                     path + 'parcellations/atlas-desikankilliany_space-fsaverage_den-164k_hemi-R.aparc-1.annot')\n",
    "dk_mni = path + 'parcellations/atlas-desikankilliany_space-MNI_res-1mm.nii.gz'\n",
    "\n",
    "# make sure label IDs are consecutive across hemispheres\n",
    "dk_fsaverage_10k = relabel_gifti(dk_fsaverage_10k)\n",
    "dk_fsaverage_164k = annot_to_gifti(dk_fsaverage_164k)  # this does relabel_gift and also converts the annot file to gifti\n",
    "\n",
    "# make the parcellaters for each space\n",
    "parcellater_fs10k = Parcellater(dk_fsaverage_10k, 'fsaverage')\n",
    "parcellater_fs164k = Parcellater(dk_fsaverage_164k, 'fsaverage')\n",
    "parcellater_mni = Parcellater(dk_mni, 'MNI152')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download enigma\n",
    "enigmamap = pd.read_csv(path+'data/ENIGMA_S32_partial_correlation_between_cortical_thickness_and_chlorpromazine_equivalents.csv')\n",
    "enigmamap.drop([68, 69], inplace=True)  # remove the last two rows\n",
    "enigma_parc = enigmamap['partial_r'].to_numpy()\n",
    "\n",
    "# download the regions for MNI152 \n",
    "rois = pd.read_csv(path+'parcellations/atlas-desikankilliany.csv')\n",
    "rois = rois[(rois['structure'] == 'cortex')].id.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sig-maps \n",
    "img_L = load_data(path + 'data/lh.sig.nii')\n",
    "img_gii_L = construct_shape_gii(img_L)\n",
    "\n",
    "img_R = load_data(path + 'data/rh.sig.nii')  \n",
    "img_gii_R = construct_shape_gii(img_R)\n",
    "\n",
    "#parcellate the turku_map into turku_parc\n",
    "turku_map = (img_gii_L, img_gii_R)\n",
    "turku_parc = parcellater_fs164k.fit_transform(turku_map, space='fsaverage', ignore_background_data=True)\n",
    "np.save(path + 'data/turku_parc.npy', turku_parc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get annotations \n",
    "annotations = list(fetch_annotation(source=['hcps1200',\n",
    "                                            'margulies2016',\n",
    "                                            'raichle',\n",
    "                                            'ding2010', \n",
    "                                            'finnema2016', \n",
    "                                            'dubois2015',\n",
    "                                            'dukart2018',\n",
    "                                            'gallezot2010',\n",
    "                                            'gallezot2017',\n",
    "                                            'hillmer2016',\n",
    "                                            'jaworska2020',\n",
    "                                            'kaller2017',\n",
    "                                            'kantonen2020',\n",
    "                                            'laurikainen2018',\n",
    "                                            'normandin2015',\n",
    "                                            'radnakrishnan2018',\n",
    "                                            'sandiego2015',\n",
    "                                            'satterthwaite2014',\n",
    "                                            'sasaki2012',\n",
    "                                            'savli2012',\n",
    "                                            'satterthwaite2014',\n",
    "                                            'smith2017',\n",
    "                                            'tuominen',\n",
    "                                            'mueller2013',\n",
    "                                            'naganawa2020',\n",
    "                                            'fazio2016']).keys())\n",
    "\n",
    "annotations.extend(fetch_annotation(source=['norgaard2021', 'beliveau2017'], space='fsaverage').keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laurituominen/anaconda3/lib/python3.10/site-packages/scipy/ndimage/_measurements.py:802: RuntimeWarning: invalid value encountered in divide\n",
      "  return sum / numpy.asanyarray(count).astype(numpy.float64)\n"
     ]
    }
   ],
   "source": [
    "# parcellate annotations\n",
    "\n",
    "# initialize\n",
    "parcellated = dict([])\n",
    "for (src, desc, space, den) in annotations:\n",
    "\n",
    "    annot = fetch_annotation(source=src, desc=desc, space=space, den=den)\n",
    "    \n",
    "    if space == 'MNI152':\n",
    "        parcellater = parcellater_mni\n",
    "    elif space == 'fsaverage' and den == '164k':\n",
    "        parcellater = parcellater_fs164k\n",
    "    elif space == 'fsLR' and den == '164k':\n",
    "        space = 'fsaverage'\n",
    "        annot = transforms.fslr_to_fsaverage(annot, target_density='164k')\n",
    "        parcellater = parcellater_fs164k\n",
    "    elif space == 'fsLR' and den != '164k':\n",
    "        # unfortunately for fsLR-4k we are upsampling to fsaverage-10k to parcellate but it should be fine\n",
    "        space = 'fsaverage'\n",
    "        annot = transforms.fslr_to_fsaverage(annot, target_density='10k')\n",
    "        parcellater = parcellater_fs10k\n",
    "\n",
    "    parcellated[desc] = parcellater.fit_transform(annot, space=space, ignore_background_data=True)\n",
    "    \n",
    "    if parcellated[desc].shape == (1,83):\n",
    "        parcellated[desc] = parcellated[desc][0][rois]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get spins \n",
    "spins = pd.read_csv(path + 'parcellations/spins_hungarian_aparc+aseg_ctx.csv', header=None)\n",
    "nspins = spins.values.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlations between parcellated annotations & parcellated antipsychotic effects on cortical thickness \n",
    "\n",
    "# initialize dictionary to save out later\n",
    "nulls_enigma = dict([])\n",
    "corrs_enigma = dict([])\n",
    "\n",
    "nulls_turku = dict([])\n",
    "corrs_turku = dict([])\n",
    "\n",
    "# go over annotations \n",
    "for src, desc, space, den in annotations:\n",
    "    if space == 'MNI152':\n",
    "        parcellation=dk_mni\n",
    "        \n",
    "    elif space == 'fsaverage' and den == '164k':\n",
    "        parcellation=dk_fsaverage_164k\n",
    "        \n",
    "    elif space == 'fsLR' and den == '164k':\n",
    "        parcellation=dk_fsaverage_164k\n",
    "        \n",
    "    elif space == 'fsLR' and den != '164k':\n",
    "        parcellation=dk_fsaverage_10k\n",
    "    \n",
    "    # correlation between annotations and enigma map\n",
    "    rho_enigma = pearsonr(parcellated[desc], enigma_parc)[0]\n",
    "    rho_turku = pearsonr(parcellated[desc], turku_parc)[0]\n",
    "\n",
    "    # get 10k rotations \n",
    "    rotated = hungarian(data=parcellated[desc], n_perm=10000, spins=spins, parcellation=parcellation) \n",
    "    \n",
    "    # get null Turku\n",
    "    n = np.zeros((nspins, ))\n",
    "    for i in range(nspins):\n",
    "        n[i] = pearsonr(turku_parc, rotated[:,i])[0]    \n",
    "    \n",
    "    # get p-value Turku\n",
    "    pspin = (1 + sum(abs(n) > abs(rho_turku ))) / (nspins + 1)\n",
    "\n",
    "    # store, multiply by -1 to make more intuitive  \n",
    "    corrs_turku[src+'_'+desc] = ( (-1 * rho_turku, pspin ) )\n",
    "    nulls_turku[src+'_'+desc] = n\n",
    "    \n",
    "    # get null enigma\n",
    "    n = np.zeros((nspins, ))\n",
    "    for i in range(nspins):\n",
    "        n[i] = pearsonr(enigma_parc, rotated[:,i])[0]    \n",
    "    \n",
    "    # get p-value enigma\n",
    "    pspin = (1 + sum(abs(n) > abs(rho_enigma ))) / (nspins + 1)\n",
    "\n",
    "    # store, multiply by -1 to make more intuitive\n",
    "    corrs_enigma[src+'_'+desc] = ( (-1 * rho_enigma, pspin ) )\n",
    "    nulls_enigma[src+'_'+desc] = n\n",
    "\n",
    "# save correlations & nulls \n",
    "np.savez(path + 'data/corrs_turku.npz', **corrs_turku)\n",
    "np.savez(path + 'data/nulls_turku.npz', **nulls_turku)\n",
    "np.savez(path + 'data/nulls_enigma.npz', **nulls_enigma)\n",
    "np.savez(path + 'data/corrs_enigma.npz', **corrs_enigma)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
