{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "576c6aee-bec9-4253-9577-8839d3c5a2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from neuromaps.images import load_data, load_gifti, annot_to_gifti, relabel_gifti, construct_shape_gii\n",
    "from neuromaps.resampling import resample_images\n",
    "from neuromaps.nulls import alexander_bloch, burt2020\n",
    "from neuromaps.parcellate import Parcellater\n",
    "from scipy.stats import pearsonr\n",
    "from neuromaps import transforms \n",
    "from neuromaps.stats import compare_images\n",
    "from neuromaps.nulls import hungarian\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import pickle \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dea46d88-b7e7-4b0b-bef3-72d7f05c9ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path\n",
    "base_path = os.path.dirname(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6f8e0d2-2e12-4281-903d-84511add5050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load turku \n",
    "turku_parc = np.load(os.path.join(base_path, 'data' 'turku_parc.npy'))\n",
    "\n",
    "# load enigma\n",
    "enigma_file='ENIGMA_S32_partial_correlation_between_cortical_thickness_and_chlorpromazine_equivalents.csv'\n",
    "enigmamap = pd.read_csv(os.path.join(base_path, 'data',enigma_file))\n",
    "enigmamap.drop([68, 69], inplace=True)  # Remove the last two rows\n",
    "enigma_parc = enigmamap['partial_r'].to_numpy()\n",
    "\n",
    "# load neurosynth\n",
    "neurosynth = pd.read_csv(os.path.join(base_path, 'data', 'atl-desikankilliany_res-83_neurosynth.csv') )\n",
    "rois = pd.read_csv(os.path.join(base_path, 'parcellations', 'atlas-desikankilliany.csv'))\n",
    "neurosynth['structure'] = rois['structure']\n",
    "neurosynth = neurosynth[neurosynth['structure'].str.match('cortex')]\n",
    "neurosynth.drop(['Unnamed: 0', 'structure'], axis=1,  inplace=True)\n",
    "\n",
    "# term that does not match\n",
    "categ = pd.read_csv(os.path.join(base_path, 'data', 'neurosynth_categories.csv'), header=None)\n",
    "categ = categ.iloc[:,0].to_list()\n",
    "idx = np.array([n in categ for n in neurosynth.columns], dtype=bool)\n",
    "neurosynth = neurosynth.iloc[:,idx]\n",
    "\n",
    "# load rois \n",
    "rois = pd.read_csv(os.path.join(base_path, 'parcellations' ,'atlas-desikankilliany.csv'))\n",
    "\n",
    "\n",
    "# get spins\n",
    "spins = pd.read_csv(os.path.join(base_path, 'parcellations','spins_hungarian_aparc+aseg_ctx.csv'), header=None)\n",
    "nspins = spins.values.shape[1]\n",
    "\n",
    "# get parcellation\n",
    "def load_atlas(base_path):\n",
    "    \"\"\"\n",
    "    Load different parcellation files.\n",
    "    \"\"\"    \n",
    "    atlas= {\n",
    "        'dk_fsaverage_10k': (\n",
    "            os.path.join(base_path, 'parcellations', 'atlas-desikankilliany_space-fsaverage_den-10k_hemi-L.label.gii.gz'),\n",
    "            os.path.join(base_path, 'parcellations', 'atlas-desikankilliany_space-fsaverage_den-10k_hemi-R.label.gii.gz')\n",
    "    ),\n",
    "    'dk_fsaverage_164k': (\n",
    "            os.path.join(base_path, 'parcellations', 'atlas-desikankilliany_space-fsaverage_den-164k_hemi-L.aparc-1.annot'),\n",
    "            os.path.join(base_path, 'parcellations', 'atlas-desikankilliany_space-fsaverage_den-164k_hemi-R.aparc-1.annot')\n",
    "    ),\n",
    "    'dk_mni': os.path.join(base_path, 'parcellations', 'atlas-desikankilliany_space-MNI_res-1mm.nii.gz')\n",
    "    }\n",
    "    return (atlas)\n",
    "\n",
    "atlas = load_atlas(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edfabb36-5c2c-42ef-b804-8c376a68335f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_corrs_neurosynth(parc):\n",
    "    \"\"\"\n",
    "    Calculate correlations & nulls between neurosynth &\n",
    "    parcellated (parc) antipsychotic effects on cortical thickness\n",
    "    \"\"\"\n",
    "\n",
    "   # initialize\n",
    "    nulls = dict([])\n",
    "    corrs = dict([])\n",
    "    \n",
    "    for column in neurosynth:\n",
    "\n",
    "        # empirical correlation between annotations and parc\n",
    "        rho = pearsonr(neurosynth[column].to_numpy(), parc)[0]\n",
    "\n",
    "        # get 10k rotations\n",
    "        rotated = hungarian(data=neurosynth[column], n_perm=10000, spins=spins, parcellation=atlas['dk_fsaverage_164k'])\n",
    "\n",
    "        # get null\n",
    "        n = np.zeros((nspins, ))\n",
    "        for i in range(nspins):\n",
    "            n[i] = pearsonr(parc, rotated[:,i])[0]    \n",
    "\n",
    "        # get p-value\n",
    "        pspin = (1 + sum(abs(n) > abs(rho ))) / (nspins + 1)\n",
    "\n",
    "        # store, multiply by -1 to make more intuitive, because smaller p-value/rho means bigger effect  \n",
    "        corrs[column] = ( (-1 * rho, pspin ) )\n",
    "        nulls[column] = n\n",
    "\n",
    "    return(nulls, corrs)\n",
    "\n",
    "nulls_turku, corrs_turku = calc_corrs_neurosynth(turku_parc)\n",
    "nulls_enigma, corrs_enigma = calc_corrs_neurosynth(enigma_parc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b80e2d3d-571f-4c83-ab6c-a27da4438bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorize, calculate means, permutate\n",
    "# code for this bit modified from here https://github.com/netneurolab/bazinet_assortativity/blob/master/main_script.py\n",
    "\n",
    "# Load information about categories\n",
    "def get_neurosynth_results(corrs): \n",
    "    \n",
    "    # initialize\n",
    "    neurosynth_results = {} \n",
    "    \n",
    "    neurosynth_results['categories_labels'] = [ 'action','learn./memory', 'other', 'emotion', 'attention',\n",
    "                                               'reason/deci. making', 'exec./cog. control', \n",
    "                                               'social function', 'perception', 'motivation', 'language' ]                                          \n",
    "    \n",
    "    # get categories for each term\n",
    "    neurosyn_categories = pd.read_csv(path + \"data/neurosynth_categories.csv\", header=None).values[:, 1].astype(int)\n",
    "    neurosynth_results['categories'] = neurosyn_categories\n",
    "    neurosynth_results['rho'] = np.array([value[0] for value in corrs.values()])\n",
    "    neurosynth_results['pspin'] = np.array([value[1] for value in corrs.values()])\n",
    "    \n",
    "    # Compute mean of each category\n",
    "    neurosynth_results['r_categories'] =  np.array([neurosynth_results['rho'][neurosyn_categories == 1+i].mean() \n",
    "                                                   for i in range(11)])\n",
    "    \n",
    "    # Compute mean after permuting categories\n",
    "    neurosynth_results['r_categories_perm'] = np.zeros((11, 10000))\n",
    "    for i in range(11):\n",
    "        for j in range(10000):\n",
    "            perm = np.random.permutation(neurosyn_categories)\n",
    "            r_perm_mean = neurosynth_results['rho'][perm == i+1].mean()\n",
    "            neurosynth_results['r_categories_perm'][i, j] = r_perm_mean\n",
    "\n",
    "    # Compute p-values by comparing means and permuted means \n",
    "    neurosynth_results['r_categories_p'] = np.zeros((11))\n",
    "    for i in range(11):\n",
    "        neurosynth_results['r_categories_p'][i] = (1 + sum( abs(neurosynth_results['r_categories_perm'][i, :]) > abs(neurosynth_results['r_categories'][i])))/ 10001\n",
    "    \n",
    "    # multiple comparisons \n",
    "    _, neurosynth_results['r_categories_p_fdr'], _, _ = multipletests(\n",
    "    neurosynth_results['r_categories_p'], method='fdr_by')\n",
    "    \n",
    "    return(neurosynth_results)\n",
    "\n",
    "neurosynth_results_turku = get_neurosynth_results(corrs_turku)\n",
    "neurosynth_results_enigma = get_neurosynth_results(corrs_enigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5f65d50d-24ba-4a6c-bcc7-1d67089c8536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "with open(path + 'data/neurosynth_results_turku.pkl', 'wb') as f:\n",
    "    # dump the dictionary into the file\n",
    "    pickle.dump(neurosynth_results_turku, f)\n",
    "    \n",
    "with open(path + 'data/neurosynth_results_enigma.pkl', 'wb') as f:\n",
    "    # dump the dictionary into the file\n",
    "    pickle.dump(neurosynth_results_enigma, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "832928ed-be09-4afc-b751-5e2a31025e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the top 10% of posivitive and top 10 % negative correlations\n",
    "def topbottom(corrs, nulls):\n",
    "    df1 = pd.DataFrame.from_dict(corrs, orient=\"index\", columns=[\"rho\", \"pspin\"])\n",
    "    df1.sort_values(by=\"rho\", inplace=True)\n",
    "    df1 = pd.concat([df1.iloc[:12], df1.iloc[-12:]])\n",
    "    index_list = list(df1.index.values)\n",
    "    new_corrs = df1.T.to_dict(orient='list')\n",
    "    \n",
    "    df2 = pd.DataFrame.from_dict(nulls)\n",
    "    df2 = df2[index_list]\n",
    "    new_nulls = df2.to_dict(orient='list')\n",
    "    return(new_corrs, new_nulls)\n",
    "\n",
    "corrs_turku_tb, nulls_turku_tb = topbottom(corrs_turku, nulls_turku)\n",
    "corrs_enigma_tb, nulls_enigma_tb = topbottom(corrs_enigma, nulls_enigma)\n",
    "\n",
    "np.savez(path + 'data/corrs_turku_neurosynth_tb.npz', **corrs_turku_tb)\n",
    "np.savez(path + 'data/nulls_turku_neurosynth_tb.npz', **nulls_turku_tb)\n",
    "np.savez(path + 'data/nulls_enigma_neurosynth_tb.npz', **nulls_enigma_tb)\n",
    "np.savez(path + 'data/corrs_enigma_neurosynth_tb.npz', **corrs_enigma_tb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
