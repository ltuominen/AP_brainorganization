{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "576c6aee-bec9-4253-9577-8839d3c5a2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from neuromaps.images import load_data, load_gifti, annot_to_gifti, relabel_gifti, construct_shape_gii\n",
    "from neuromaps.resampling import resample_images\n",
    "from neuromaps.nulls import alexander_bloch, burt2020\n",
    "from neuromaps.parcellate import Parcellater\n",
    "from scipy.stats import pearsonr\n",
    "from neuromaps import transforms \n",
    "from neuromaps.stats import compare_images\n",
    "from neuromaps.nulls import hungarian\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import pickle \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dea46d88-b7e7-4b0b-bef3-72d7f05c9ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path\n",
    "path = '/Users/laurituominen/Documents/Research/Reettis/neuromaps/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6f8e0d2-2e12-4281-903d-84511add5050",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not GiftiImage",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# get parcellation\u001b[39;00m\n\u001b[1;32m     30\u001b[0m dk_fsaverage_164k \u001b[38;5;241m=\u001b[39m (path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparcellations/atlas-desikankilliany_space-fsaverage_den-164k_hemi-L.aparc-1.annot\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     31\u001b[0m                      path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparcellations/atlas-desikankilliany_space-fsaverage_den-164k_hemi-R.aparc-1.annot\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m dk_fsaverage_164k \u001b[38;5;241m=\u001b[39m \u001b[43mannot_to_gifti\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdk_fsaverage_164k\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# this does relabel_gift and also converts the annot file to gifti\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/neuromaps/images.py:529\u001b[0m, in \u001b[0;36mannot_to_gifti\u001b[0;34m(parcellation, background)\u001b[0m\n\u001b[1;32m    525\u001b[0m         labeltable\u001b[38;5;241m.\u001b[39mlabels\u001b[38;5;241m.\u001b[39mappend(glabel)\n\u001b[1;32m    527\u001b[0m     gifti \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (nib\u001b[38;5;241m.\u001b[39mGiftiImage(darrays\u001b[38;5;241m=\u001b[39m[darr], labeltable\u001b[38;5;241m=\u001b[39mlabeltable),)\n\u001b[0;32m--> 529\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrelabel_gifti\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgifti\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackground\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackground\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/neuromaps/images.py:456\u001b[0m, in \u001b[0;36mrelabel_gifti\u001b[0;34m(parcellation, background, offset)\u001b[0m\n\u001b[1;32m    452\u001b[0m     background \u001b[38;5;241m=\u001b[39m PARCIGNORE\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hemi \u001b[38;5;129;01min\u001b[39;00m parcellation:\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;66;03m# get necessary info from file\u001b[39;00m\n\u001b[0;32m--> 456\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mload_gifti\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhemi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    457\u001b[0m     data \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39magg_data()\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    458\u001b[0m     labels \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mlabeltable\u001b[38;5;241m.\u001b[39mlabels\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/neuromaps/images.py:170\u001b[0m, in \u001b[0;36mload_gifti\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# it's not a pre-loaded GiftiImage so error out\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(err, \u001b[38;5;167;01mTypeError\u001b[39;00m)\n\u001b[1;32m    168\u001b[0m           \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(err) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstat: path should be string, bytes, os.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    169\u001b[0m                               \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPathLike or integer, not GiftiImage\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 170\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/neuromaps/images.py:160\u001b[0m, in \u001b[0;36mload_gifti\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;124;03mLoad gifti file `img`.\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m    Loaded GIFTI images\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mnib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ImageFileError, \u001b[38;5;167;01mTypeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# it's gzipped, so read the gzip and pipe it in\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(err, ImageFileError) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(err)\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.gii.gz\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/nibabel/loadsave.py:96\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filename, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(filename: FileSpec, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FileBasedImage:\n\u001b[1;32m     82\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Load file given filename, guessing at file type\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03m       Image of guessed type\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[43m_stringify_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;66;03m# Check file exists and is not empty\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/nibabel/filename_parser.py:41\u001b[0m, in \u001b[0;36m_stringify_path\u001b[0;34m(filepath_or_buffer)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_stringify_path\u001b[39m(filepath_or_buffer: FileSpec) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m     26\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Attempt to convert a path-like object to a string.\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    https://github.com/pandas-dev/pandas/blob/325dd68/pandas/io/common.py#L131-L160\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpathlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mexpanduser()\u001b[38;5;241m.\u001b[39mas_posix()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/pathlib.py:960\u001b[0m, in \u001b[0;36mPath.__new__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m Path:\n\u001b[1;32m    959\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m WindowsPath \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnt\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m PosixPath\n\u001b[0;32m--> 960\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_parts\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flavour\u001b[38;5;241m.\u001b[39mis_supported:\n\u001b[1;32m    962\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot instantiate \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m on your system\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    963\u001b[0m                               \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/pathlib.py:594\u001b[0m, in \u001b[0;36mPurePath._from_parts\u001b[0;34m(cls, args)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_from_parts\u001b[39m(\u001b[38;5;28mcls\u001b[39m, args):\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;66;03m# We need to call _parse_args on the instance, so as to get the\u001b[39;00m\n\u001b[1;32m    592\u001b[0m     \u001b[38;5;66;03m# right flavour.\u001b[39;00m\n\u001b[1;32m    593\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m--> 594\u001b[0m     drv, root, parts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_drv \u001b[38;5;241m=\u001b[39m drv\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_root \u001b[38;5;241m=\u001b[39m root\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/pathlib.py:578\u001b[0m, in \u001b[0;36mPurePath._parse_args\u001b[0;34m(cls, args)\u001b[0m\n\u001b[1;32m    576\u001b[0m     parts \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39m_parts\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 578\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;66;03m# Force-cast str subclasses to str (issue #21127)\u001b[39;00m\n\u001b[1;32m    581\u001b[0m         parts\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mstr\u001b[39m(a))\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not GiftiImage"
     ]
    }
   ],
   "source": [
    "# load turku \n",
    "turku_parc = np.load(path + 'data/turku_parc.npy')\n",
    "\n",
    "# load enigma\n",
    "enigmamap = pd.read_csv(path+'data/ENIGMA_S32_partial_correlation_between_cortical_thickness_and_chlorpromazine_equivalents.csv')\n",
    "enigmamap.drop([68, 69], inplace=True)  # remove the last two rows\n",
    "enigma_parc = enigmamap['partial_r'].to_numpy()\n",
    "\n",
    "# load neurosynth\n",
    "neurosynth = pd.read_csv(path+'data/atl-desikankilliany_res-83_neurosynth.csv') \n",
    "rois = pd.read_csv(path+'parcellations/atlas-desikankilliany.csv')\n",
    "neurosynth['structure'] = rois['structure']\n",
    "neurosynth = neurosynth[neurosynth['structure'].str.match('cortex')]\n",
    "neurosynth.drop(['Unnamed: 0', 'structure'], axis=1,  inplace=True)\n",
    "\n",
    "# term that does not match\n",
    "categ = pd.read_csv(path + \"data/neurosynth_categories.csv\", header=None)\n",
    "categ = categ.iloc[:,0].to_list()\n",
    "idx = np.array([n in categ for n in neurosynth.columns], dtype=bool)\n",
    "neurosynth = neurosynth.iloc[:,idx]\n",
    "\n",
    "# load rois \n",
    "rois = pd.read_csv(path+'parcellations/atlas-desikankilliany.csv')\n",
    "\n",
    "# get spins \n",
    "spins = pd.read_csv(path + 'parcellations/spins_hungarian_aparc+aseg_ctx.csv', header=None)\n",
    "nspins = spins.values.shape[1]\n",
    "\n",
    "# get parcellation\n",
    "dk_fsaverage_164k = (path + 'parcellations/atlas-desikankilliany_space-fsaverage_den-164k_hemi-L.aparc-1.annot',\n",
    "                     path + 'parcellations/atlas-desikankilliany_space-fsaverage_den-164k_hemi-R.aparc-1.annot')\n",
    "dk_fsaverage_164k = annot_to_gifti(dk_fsaverage_164k)  # this does relabel_gift and also converts the annot file to gifti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edfabb36-5c2c-42ef-b804-8c376a68335f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/Users/laurituominen/Documents/Research/Reettis/neuromaps/parcellations/atlas-desikankilliany_space-fsaverage_den-164k_hemi-L.aparc-1.annot',\n",
       " '/Users/laurituominen/Documents/Research/Reettis/neuromaps/parcellations/atlas-desikankilliany_space-fsaverage_den-164k_hemi-R.aparc-1.annot')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dk_fsaverage_164k = (path + 'parcellations/atlas-desikankilliany_space-fsaverage_den-164k_hemi-L.aparc-1.annot',\n",
    "                     path + 'parcellations/atlas-desikankilliany_space-fsaverage_den-164k_hemi-R.aparc-1.annot')\n",
    "dk_fsaverage_164k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e7b9de-4a9a-4ab5-be85-7d03f56fd70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize dictionary to save out later\n",
    "nulls_enigma = dict([])\n",
    "corrs_enigma = dict([])\n",
    "\n",
    "nulls_turku = dict([])\n",
    "corrs_turku = dict([])\n",
    "\n",
    "# go over annotations \n",
    "for column in neurosynth:\n",
    "    \n",
    "    # correlation between annotations and enigma map\n",
    "    rho_enigma = pearsonr(neurosynth[column].to_numpy(), enigma_parc)[0]\n",
    "    rho_turku = pearsonr(neurosynth[column].to_numpy(), turku_parc)[0]\n",
    "\n",
    "    # get 10k rotations \n",
    "    rotated = hungarian(data=neurosynth[column], n_perm=10000, spins=spins, parcellation=dk_fsaverage_164k) \n",
    "    \n",
    "    # get null Turku\n",
    "    n = np.zeros((nspins, ))\n",
    "    for i in range(nspins):\n",
    "        n[i] = pearsonr(turku_parc, rotated[:,i])[0]    \n",
    "    \n",
    "    # get p-value Turku\n",
    "    pspin = (1 + sum(abs(n) > abs(rho_turku ))) / (nspins + 1)\n",
    "\n",
    "    # store, multiply by -1 to make more intuitive  \n",
    "    corrs_turku[column] = ( (-1 * rho_turku, pspin ) )\n",
    "    nulls_turku[column] = n\n",
    "    \n",
    "    # get null enigma\n",
    "    n = np.zeros((nspins, ))\n",
    "    for i in range(nspins):\n",
    "        n[i] = pearsonr(enigma_parc, rotated[:,i])[0]    \n",
    "    \n",
    "    # get p-value enigma\n",
    "    pspin = (1 + sum(abs(n) > abs(rho_enigma ))) / (nspins + 1)\n",
    "\n",
    "    # store, multiply by -1 to make more intuitive\n",
    "    corrs_enigma[column] = ( (-1 * rho_enigma, pspin ) )\n",
    "    nulls_enigma[column] = n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b80e2d3d-571f-4c83-ab6c-a27da4438bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorize, calculate means, permutate\n",
    "# code for this bit modified from here https://github.com/netneurolab/bazinet_assortativity/blob/master/main_script.py\n",
    "\n",
    "# Load information about categories\n",
    "def get_neurosynth_results(corrs): \n",
    "    \n",
    "    # initialize\n",
    "    neurosynth_results = {} \n",
    "    \n",
    "    neurosynth_results['categories_labels'] = [ 'action','learn./memory', 'other', 'emotion', 'attention',\n",
    "                                               'reason/deci. making', 'exec./cog. control', \n",
    "                                               'social function', 'perception', 'motivation', 'language' ]                                          \n",
    "    \n",
    "    # get categories for each term\n",
    "    neurosyn_categories = pd.read_csv(path + \"data/neurosynth_categories.csv\", header=None).values[:, 1].astype(int)\n",
    "    neurosynth_results['categories'] = neurosyn_categories\n",
    "    neurosynth_results['rho'] = np.array([value[0] for value in corrs.values()])\n",
    "    neurosynth_results['pspin'] = np.array([value[1] for value in corrs.values()])\n",
    "    \n",
    "    # Compute mean of each category\n",
    "    neurosynth_results['r_categories'] =  np.array([neurosynth_results['rho'][neurosyn_categories == 1+i].mean() \n",
    "                                                   for i in range(11)])\n",
    "    \n",
    "    # Compute mean after permuting categories\n",
    "    neurosynth_results['r_categories_perm'] = np.zeros((11, 10000))\n",
    "    for i in range(11):\n",
    "        for j in range(10000):\n",
    "            perm = np.random.permutation(neurosyn_categories)\n",
    "            r_perm_mean = neurosynth_results['rho'][perm == i+1].mean()\n",
    "            neurosynth_results['r_categories_perm'][i, j] = r_perm_mean\n",
    "\n",
    "    # Compute p-values by comparing means and permuted means \n",
    "    neurosynth_results['r_categories_p'] = np.zeros((11))\n",
    "    for i in range(11):\n",
    "        neurosynth_results['r_categories_p'][i] = (1 + sum( abs(neurosynth_results['r_categories_perm'][i, :]) > abs(neurosynth_results['r_categories'][i])))/ 10001\n",
    "    \n",
    "    # multiple comparisons \n",
    "    _, neurosynth_results['r_categories_p_fdr'], _, _ = multipletests(\n",
    "    neurosynth_results['r_categories_p'], method='fdr_by')\n",
    "    \n",
    "    return(neurosynth_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5f65d50d-24ba-4a6c-bcc7-1d67089c8536",
   "metadata": {},
   "outputs": [],
   "source": [
    "neurosynth_results_turku = get_neurosynth_results(corrs_turku)\n",
    "neurosynth_results_enigma = get_neurosynth_results(corrs_enigma)\n",
    "\n",
    "with open(path + 'data/neurosynth_results_turku.pkl', 'wb') as f:\n",
    "    # dump the dictionary into the file\n",
    "    pickle.dump(neurosynth_results_turku, f)\n",
    "    \n",
    "with open(path + 'data/neurosynth_results_enigma.pkl', 'wb') as f:\n",
    "    # dump the dictionary into the file\n",
    "    pickle.dump(neurosynth_results_enigma, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "832928ed-be09-4afc-b751-5e2a31025e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the top 10% of posivitive and top 10 % negative correlations\n",
    "def topbottom(corrs, nulls):\n",
    "    df1 = pd.DataFrame.from_dict(corrs, orient=\"index\", columns=[\"rho\", \"pspin\"])\n",
    "    df1.sort_values(by=\"rho\", inplace=True)\n",
    "    df1 = pd.concat([df1.iloc[:12], df1.iloc[-12:]])\n",
    "    index_list = list(df1.index.values)\n",
    "    new_corrs = df1.T.to_dict(orient='list')\n",
    "    \n",
    "    df2 = pd.DataFrame.from_dict(nulls)\n",
    "    df2 = df2[index_list]\n",
    "    new_nulls = df2.to_dict(orient='list')\n",
    "    return(new_corrs, new_nulls)\n",
    "\n",
    "corrs_turku_tb, nulls_turku_tb = topbottom(corrs_turku, nulls_turku)\n",
    "corrs_enigma_tb, nulls_enigma_tb = topbottom(corrs_enigma, nulls_enigma)\n",
    "\n",
    "np.savez(path + 'data/corrs_turku_neurosynth_tb.npz', **corrs_turku_tb)\n",
    "np.savez(path + 'data/nulls_turku_neurosynth_tb.npz', **nulls_turku_tb)\n",
    "np.savez(path + 'data/nulls_enigma_neurosynth_tb.npz', **nulls_enigma_tb)\n",
    "np.savez(path + 'data/corrs_enigma_neurosynth_tb.npz', **corrs_enigma_tb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
